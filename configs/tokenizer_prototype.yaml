# Prototype config for tokenizer - 512x512x32 grid, 64x64 tokens
# Memory: ~11GB GPU

# ROI bounds (metres)
x_min: -51.2
x_max: 51.2
y_min: -51.2
y_max: 51.2
z_min: -3.0
z_max: 3.0

# Voxel grid
voxel_grid_xy: 512
voxel_grid_z: 32
max_points_per_voxel: 35

# Encoder
voxel_feat_dim: 16
bev_feat_dim: 64
patch_size: 4
enc_embed_dim: 128
enc_stage1_depth: 2
enc_stage1_heads: 8
enc_stage2_dim: 256
enc_stage2_depth: 6
enc_stage2_heads: 16
window_size: 8
mlp_ratio: 4.0
drop_path_rate: 0.1
use_checkpoint: true  # Enable gradient checkpointing for memory

# VQ (Paper: memory bank + K-Means re-init for dead codes)
vq_dim: 256
vq_codebook_size: 1024
vq_codebook_dim: 1024
vq_commitment_cost: 0.25  # Paper: lambda_1 = 0.25
vq_codebook_cost: 1.0     # Paper: lambda_2 = 1.0
vq_kmeans_iters: 10
vq_dead_threshold: 256    # iterations before code is "dead"
vq_dead_percentage: 0.03  # 3% threshold for re-init
vq_min_iterations: 200    # min iterations before re-init

# Decoder
dec_stage1_depth: 6
dec_stage1_heads: 16
dec_stage2_depth: 2
dec_stage2_heads: 8
dec_output_dim: 128

# NFG (Neural Feature Grid)
nfg_feat_dim: 16
nfg_upsample_factor: 2
nfg_z_bins: 32  # same as voxel_grid_z
nfg_mlp_hidden: 32  # Paper: hidden dimension is 32
num_depth_samples: 64

# Rays
rays_per_frame: 2048
ray_chunk_size: 256

# Spatial skip
skip_upsample_factor: 4

# Loss
surface_conc_eps: 0.4  # Paper: margin epsilon = 0.4 meters

# Data (KITTI Odometry: sequences 00-10 have ground truth poses, 11-21 do not)
kitti_root: "data/kitti/pykitti"
train_sequences: ["00", "01", "02", "03", "04", "05", "06", "07", "08"]
val_sequences: ["09"]
test_sequences: ["10"]

# Training
batch_size: 4
grad_accum_steps: 4  # Effective batch size = 16
lr: 1.0e-4
weight_decay: 0.01
warmup_steps: 2000
max_steps: 200000
amp: true

# Checkpointing
save_every_steps: 2000
eval_every_steps: 1000
num_eval_batches: 20
output_dir: "outputs/tokenizer_prototype"
