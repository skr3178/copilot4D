# CoPilot4D Tokenizer Config - Increased Depth Resolution + Depth-Weighted Loss
# Optimized for continued training from step 100000
# 
# Changes from tokenizer_memory_efficient.yaml:
#   - num_depth_samples: 128 -> 256 (2x increase for better depth accuracy)
#   - batch_size: 4 -> 2 (to accommodate more depth samples)
#   - grad_accum_steps: 8 -> 16 (maintain effective batch size 32)
#   - lr: 1.0e-4 -> 5.0e-5 (lower for fine-tuning)
#   - warmup_steps: 2000 (increased for loss change adaptation)
#   - NEW: Depth-weighted loss with gradual ramp
#
# Target GPU Memory: ~9-10GB (fits safely in 12GB GPU)
# Expected Improvement: 25-40% better accuracy at 20-50m range

# ROI bounds (metres)
x_min: -40.0
x_max: 40.0
y_min: -40.0
y_max: 40.0
z_min: -3.0
z_max: 3.0

# Voxel grid
voxel_grid_xy: 512
voxel_grid_z: 16
max_points_per_voxel: 35

# Encoder
voxel_feat_dim: 16
bev_feat_dim: 32
patch_size: 4
enc_embed_dim: 64
enc_stage1_depth: 2
enc_stage1_heads: 4
enc_stage2_dim: 128
enc_stage2_depth: 4
enc_stage2_heads: 8
window_size: 8
mlp_ratio: 4.0
drop_path_rate: 0.1
use_checkpoint: true

# VQ (Paper: memory bank + K-Means re-init for dead codes)
vq_dim: 128
vq_codebook_size: 1024
vq_codebook_dim: 512
vq_commitment_cost: 1.0
vq_codebook_cost: 0.25
vq_kmeans_iters: 10
vq_dead_threshold: 256
vq_dead_percentage: 0.03
vq_min_iterations: 200

# Decoder
dec_stage1_depth: 4
dec_stage1_heads: 8
dec_stage2_depth: 2
dec_stage2_heads: 4
dec_output_dim: 64

# NFG (Neural Feature Grid) - INCREASED DEPTH SAMPLES
nfg_feat_dim: 16
nfg_upsample_factor: 2
nfg_z_bins: 16
nfg_mlp_hidden: 32
num_depth_samples: 256  # Increased from 128 (2x for finer depth resolution)

# Rays
rays_per_frame: 2048
ray_chunk_size: 64      # Reduced from 96 for finer memory management with larger depth samples

# Spatial skip
skip_upsample_factor: 4

# Loss
surface_conc_eps: 0.4

# NEW: Depth-weighted loss parameters
# 
# CRITICAL: We use DIRECT depth weighting (depth^alpha) to emphasize FAR FIELD (30-50m)
# NOT inverse weighting (1/depth^alpha) which would emphasize near field.
#
# With alpha=0.5:
#   - 10m depth: weight = sqrt(10) ≈ 3.16
#   - 50m depth: weight = sqrt(50) ≈ 7.07
#   - Far field gets ~2.2x more gradient emphasis
#
# Weights are normalized to sum to 1.0 to maintain stable total loss magnitude
# during the ramp (prevents destabilizing VQ and surface concentration).
#
# Ramp schedule:
#   Step 100000: 90% absolute, 10% relative (total = 1.0)
#   Step 102500: 75% absolute, 25% relative (total = 1.0)
#   Step 105000: 50% absolute, 50% relative (total = 1.0, target reached)

depth_loss_type: "combined"         # Options: "l1", "weighted", "relative", "combined"
depth_loss_alpha: 0.5               # depth^α weighting (0=uniform, 0.5=emphasizes far field)
depth_loss_absolute_weight: 0.9     # Starting weight for absolute (will decrease proportionally)
depth_loss_relative_weight: 0.1     # Starting weight for relative (will increase to 0.9)
depth_loss_ramp_steps: 5000         # Steps to ramp: 100000-105000

# Data (KITTI Odometry)
kitti_root: "data/kitti/pykitti"
train_sequences: ["00", "01", "02", "03", "04", "05", "06", "07", "08"]
val_sequences: ["09"]
test_sequences: ["10"]

# Training - ADJUSTED for increased depth resolution and new loss
batch_size: 2           # Reduced from 4 to accommodate 2x depth samples
grad_accum_steps: 16    # Increased from 8 (maintain effective batch 32)
lr: 5.0e-5              # Reduced from 1.0e-4 for fine-tuning from step 100000
weight_decay: 0.01
warmup_steps: 2000      # Increased from 1000 for loss change adaptation
max_steps: 200000
amp: true

# Checkpointing
save_every_steps: 10000
eval_every_steps: 1000
num_eval_batches: 10
output_dir: "outputs/tokenizer_memory_efficient"

# Expected Performance:
#   Memory usage: ~9-10GB GPU
#   Training speed: Similar (same effective batch size)
#   Depth accuracy: 25-40% improvement at 20-50m range
#   
# Loss Ramp Schedule (first 5000 steps):
#   Step 100000-105000: relative_weight increases 0.1 -> 1.0
#   Step 105000+: relative_weight fixed at 1.0
#
# To resume training from step 100000:
#   python scripts/train_tokenizer.py \
#       --config configs/tokenizer_increased_depth.yaml \
#       --resume outputs/tokenizer_memory_efficient/checkpoint_step_100000.pt
