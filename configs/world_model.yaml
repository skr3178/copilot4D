# CoPilot4D World Model Configuration
# Full configuration matching paper specifications

# --- Tokenizer interface ---
codebook_size: 1024
token_grid_h: 128
token_grid_w: 128

# --- Sequence ---
num_frames: 6              # 3 past + 3 future for initial dev
num_past_frames: 3

# --- U-Net architecture ---
level_dims: [256, 384, 512]
level_heads: [8, 12, 16]
level_windows: [8, 8, 16]
head_dim: 32
enc_st_blocks: [2, 2, 1]   # ST-blocks per encoder level
dec_st_blocks: [1, 2]       # ST-blocks per decoder level
mlp_ratio: 4.0
drop_path_rate: 0.0

# --- Action conditioning ---
action_dim: 16             # 4x4 SE(3) flattened

# --- Discrete diffusion ---
mask_schedule: "cosine"    # gamma(u) = cos(u * pi/2)
noise_eta: 20.0            # eta% uniform noise
label_smoothing: 0.1
prob_future_pred: 0.5
prob_joint_denoise: 0.4
prob_individual_denoise: 0.1

# --- Inference ---
num_sampling_steps: 10
cfg_weight: 2.0
choice_temperature: 4.5

# --- Optimization (paper A.3) ---
lr: 0.001
beta1: 0.9
beta2: 0.95
weight_decay: 0.0001
warmup_steps: 2000
max_steps: 750000
cosine_min_ratio: 0.1
batch_size: 8
grad_clip: 5.0
amp: true

# --- Data ---
kitti_root: "data/kitti/pykitti"
tokenizer_checkpoint: "outputs/tokenizer/checkpoint_latest.pt"
token_dir: "outputs/tokens"
train_sequences: ["00", "01", "02", "03", "04", "05", "06", "07", "08"]
val_sequences: ["09"]
test_sequences: ["10"]

# --- Checkpointing ---
save_every_steps: 5000
eval_every_steps: 2500
log_every_steps: 100
output_dir: "outputs/world_model"
